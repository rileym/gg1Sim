# gg1Sim

	This is a G/G/1 queue simulation I wrote for a simulation class (See http://en.wikipedia.org/wiki/G/G/1_queue). The purpose of the exercise was to demonstrate the potential sensitivity of certain performance metrics to the choice of "input distribution” in a given simulation. In this case the metric was mean queue length and the input distribution was the distribution of inter-arrival times. I had learned basic python the week before this assignment so took the exercise as an opportunity to further familiarize myself with python in general and object oriented pyhton specifically. Note too that we were not allowed to use any high level packages or routines in this assignment, e.g., no numpy, no scipy, and had to generate our own random variables.

Specifics of the queue simulation:

	We were to estimate L (the average queue length through time) for G/G/1 queues with service times generated by the sum of two independent uniform random variables (the distribution looks like a triangle) and with each of the following inter-arrival time distributions: exponential, weibul (2x: once for each of two different parameters), and auto-correlated normal random variables (2x: once for positive and once for negative correlation). If I remember correctly, the parameters are calibrated such that the inter-arrival times for the various distributions have about the same mean times; however, —- and this is the moral of the exercise to some extent —-, the differences in the shape of these distributions or their non-independence leads to very different values of L, i.e., the performance metric is sensitive to the choice input distribution.

	The formal model behind this simulation is a "generalized semi-markov process" (GSMP) which is a stochastic process description of a discrete event system, if you are interested in further.




